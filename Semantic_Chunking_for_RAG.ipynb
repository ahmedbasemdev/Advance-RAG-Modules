{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc257add3b0241d98c9c91e18ef8e63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e835691081264d509f36893e6d7d9f65",
              "IPY_MODEL_d365ad1fe5414d7eb61aa36133a15834",
              "IPY_MODEL_d0f576a5c2e746769f03e8befb901234"
            ],
            "layout": "IPY_MODEL_811e48752ee84432aa94f0f28d131337"
          }
        },
        "e835691081264d509f36893e6d7d9f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19340cb3e7e24ffa8281521abdf25efe",
            "placeholder": "​",
            "style": "IPY_MODEL_0aba08a0af8443e4afddc6f520a4dd57",
            "value": "Fetching 5 files: 100%"
          }
        },
        "d365ad1fe5414d7eb61aa36133a15834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52e19f535faf46de9c3f73c581d1cad9",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce2201a872e64cb59d23049f4bcd9c7f",
            "value": 5
          }
        },
        "d0f576a5c2e746769f03e8befb901234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0688044a024aafa8e97aebbded8414",
            "placeholder": "​",
            "style": "IPY_MODEL_df49b9e9b9514436b513b2deb483a32b",
            "value": " 5/5 [00:00&lt;00:00, 268.20it/s]"
          }
        },
        "811e48752ee84432aa94f0f28d131337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19340cb3e7e24ffa8281521abdf25efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aba08a0af8443e4afddc6f520a4dd57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52e19f535faf46de9c3f73c581d1cad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce2201a872e64cb59d23049f4bcd9c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df0688044a024aafa8e97aebbded8414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df49b9e9b9514436b513b2deb483a32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Semantic chunking involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together."
      ],
      "metadata": {
        "id": "Rdclb__Ev6T7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k6OCleiNvv8u"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain_experimental langchain_openai langchain_community langchain ragas chromadb langchain-groq fastembed pypdf openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://arxiv.org/pdf/1810.04805.pdf\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K96U__PpwRBQ",
        "outputId": "3fe07711-9765-41da-943e-2c9128a710f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-05 13:08:37--  https://arxiv.org/pdf/1810.04805.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.131.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/1810.04805 [following]\n",
            "--2024-09-05 13:08:37--  http://arxiv.org/pdf/1810.04805\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 775166 (757K) [application/pdf]\n",
            "Saving to: ‘1810.04805.pdf.1’\n",
            "\n",
            "1810.04805.pdf.1    100%[===================>] 757.00K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-09-05 13:08:37 (10.0 MB/s) - ‘1810.04805.pdf.1’ saved [775166/775166]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#\n",
        "loader = PyPDFLoader(\"1810.04805.pdf\")\n",
        "documents = loader.load()\n",
        "#\n",
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmUcN9XGwSrB",
        "outputId": "c0223ba4-525a-4eed-d20c-fbbace64a6cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Native Chunking(RecursiveCharacterTextSplitting)"
      ],
      "metadata": {
        "id": "SQmIjtlwwxyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False)\\\n",
        "\n",
        "naive_chunks = text_splitter.split_documents(documents)\n",
        "for chunk in naive_chunks[10:15]:\n",
        "  print(chunk.page_content+ \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCiTb1gewWrD",
        "outputId": "cf7cb765-8173-4b9a-a4b6-161a97b59b98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT BERT \n",
            "E[CLS] E1 E[SEP] ... ENE1’... EM’\n",
            "C\n",
            "T1\n",
            "T[SEP] ...\n",
            " TN\n",
            "T1’...\n",
            " TM’\n",
            "[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \n",
            "Question Paragraph Start/End Span \n",
            "BERT \n",
            "E[CLS] E1 E[SEP] ... ENE1’... EM’\n",
            "C\n",
            "T1\n",
            "T[SEP] ...\n",
            " TN\n",
            "T1’...\n",
            " TM’\n",
            "[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \n",
            "Masked Sentence A Masked Sentence B \n",
            "Pre-training Fine-Tuning NSP Mask LM Mask LM \n",
            "Unlabeled Sentence A and B Pair SQuAD \n",
            "Question Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\n",
            "tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\n",
            "models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\n",
            "symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\n",
            "tions/answers).\n",
            "ing and auto-encoder objectives have been used\n",
            "for pre-training such models (Howard and Ruder,\n",
            "\n",
            "2018; Radford et al., 2018; Dai and Le, 2015).\n",
            "2.3 Transfer Learning from Supervised Data\n",
            "There has also been work showing effective trans-\n",
            "fer from supervised tasks with large datasets, such\n",
            "as natural language inference (Conneau et al.,\n",
            "2017) and machine translation (McCann et al.,\n",
            "2017). Computer vision research has also demon-\n",
            "strated the importance of transfer learning from\n",
            "large pre-trained models, where an effective recipe\n",
            "is to ﬁne-tune models pre-trained with Ima-\n",
            "geNet (Deng et al., 2009; Yosinski et al., 2014).\n",
            "3 BERT\n",
            "We introduce BERT and its detailed implementa-\n",
            "tion in this section. There are two steps in our\n",
            "framework: pre-training and ﬁne-tuning . Dur-\n",
            "ing pre-training, the model is trained on unlabeled\n",
            "data over different pre-training tasks. For ﬁne-\n",
            "tuning, the BERT model is ﬁrst initialized with\n",
            "the pre-trained parameters, and all of the param-\n",
            "eters are ﬁne-tuned using labeled data from the\n",
            "downstream tasks. Each downstream task has sep-\n",
            "\n",
            "arate ﬁne-tuned models, even though they are ini-\n",
            "tialized with the same pre-trained parameters. The\n",
            "question-answering example in Figure 1 will serve\n",
            "as a running example for this section.\n",
            "A distinctive feature of BERT is its uniﬁed ar-\n",
            "chitecture across different tasks. There is mini-mal difference between the pre-trained architec-\n",
            "ture and the ﬁnal downstream architecture.\n",
            "Model Architecture BERT’s model architec-\n",
            "ture is a multi-layer bidirectional Transformer en-\n",
            "coder based on the original implementation de-\n",
            "scribed in Vaswani et al. (2017) and released in\n",
            "thetensor2tensor library.1Because the use\n",
            "of Transformers has become common and our im-\n",
            "plementation is almost identical to the original,\n",
            "we will omit an exhaustive background descrip-\n",
            "tion of the model architecture and refer readers to\n",
            "Vaswani et al. (2017) as well as excellent guides\n",
            "such as “The Annotated Transformer.”2\n",
            "In this work, we denote the number of layers\n",
            "(i.e., Transformer blocks) as L, the hidden size as\n",
            "\n",
            "H, and the number of self-attention heads as A.3\n",
            "We primarily report results on two model sizes:\n",
            "BERT BASE (L=12, H=768, A=12, Total Param-\n",
            "eters=110M) and BERT LARGE (L=24, H=1024,\n",
            "A=16, Total Parameters=340M).\n",
            "BERT BASE was chosen to have the same model\n",
            "size as OpenAI GPT for comparison purposes.\n",
            "Critically, however, the BERT Transformer uses\n",
            "bidirectional self-attention, while the GPT Trans-\n",
            "former uses constrained self-attention where every\n",
            "token can only attend to context to its left.4\n",
            "1https://github.com/tensorﬂow/tensor2tensor\n",
            "2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
            "3In all cases we set the feed-forward/ﬁlter size to be 4H,\n",
            "i.e., 3072 for the H= 768 and 4096 for the H= 1024 .\n",
            "4We note that in the literature the bidirectional Trans-\n",
            "\n",
            "Input/Output Representations To make BERT\n",
            "handle a variety of down-stream tasks, our input\n",
            "representation is able to unambiguously represent\n",
            "both a single sentence and a pair of sentences\n",
            "(e.g.,⟨Question, Answer⟩) in one token sequence.\n",
            "Throughout this work, a “sentence” can be an arbi-\n",
            "trary span of contiguous text, rather than an actual\n",
            "linguistic sentence. A “sequence” refers to the in-\n",
            "put token sequence to BERT, which may be a sin-\n",
            "gle sentence or two sentences packed together.\n",
            "We use WordPiece embeddings (Wu et al.,\n",
            "2016) with a 30,000 token vocabulary. The ﬁrst\n",
            "token of every sequence is always a special clas-\n",
            "siﬁcation token ( [CLS] ). The ﬁnal hidden state\n",
            "corresponding to this token is used as the ag-\n",
            "gregate sequence representation for classiﬁcation\n",
            "tasks. Sentence pairs are packed together into a\n",
            "single sequence. We differentiate the sentences in\n",
            "two ways. First, we separate them with a special\n",
            "token ( [SEP] ). Second, we add a learned embed-\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.fastembed import FastEmbedEmbeddings\n",
        "embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cc257add3b0241d98c9c91e18ef8e63b",
            "e835691081264d509f36893e6d7d9f65",
            "d365ad1fe5414d7eb61aa36133a15834",
            "d0f576a5c2e746769f03e8befb901234",
            "811e48752ee84432aa94f0f28d131337",
            "19340cb3e7e24ffa8281521abdf25efe",
            "0aba08a0af8443e4afddc6f520a4dd57",
            "52e19f535faf46de9c3f73c581d1cad9",
            "ce2201a872e64cb59d23049f4bcd9c7f",
            "df0688044a024aafa8e97aebbded8414",
            "df49b9e9b9514436b513b2deb483a32b"
          ]
        },
        "id": "gyd-9i_6xCph",
        "outputId": "b315705b-4a52-43aa-c82d-9763af8a168b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc257add3b0241d98c9c91e18ef8e63b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(embed_model, breakpoint_threshold_type=\"percentile\")\n",
        "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
        "\n",
        "for semantic_chunk in semantic_chunks:\n",
        "  if \"Effect of Pre-training Tasks\" in semantic_chunk.page_content:\n",
        "    print(semantic_chunk.page_content)\n",
        "    print(len(semantic_chunk.page_content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq-x7M9FWs8v",
        "outputId": "1b25f071-a839-4644-f8db-99451ca6416d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev Set\n",
            "Tasks MNLI-m QNLI MRPC SST-2 SQuAD\n",
            "(Acc) (Acc) (Acc) (Acc) (F1)\n",
            "BERT BASE 84.4 88.4 86.7 92.7 88.5\n",
            "No NSP 83.9 84.9 86.5 92.6 87.9\n",
            "LTR & No NSP 82.1 84.3 77.5 92.1 77.8\n",
            "+ BiLSTM 82.1 84.1 75.7 91.6 84.9\n",
            "Table 5: Ablation over the pre-training tasks using the\n",
            "BERT BASE architecture. “No NSP” is trained without\n",
            "the next sentence prediction task. “LTR & No NSP” is\n",
            "trained as a left-to-right LM without the next sentence\n",
            "prediction, like OpenAI GPT. “+ BiLSTM” adds a ran-\n",
            "domly initialized BiLSTM on top of the “LTR + No\n",
            "NSP” model during ﬁne-tuning. ablation studies can be found in Appendix C. 5.1 Effect of Pre-training Tasks\n",
            "We demonstrate the importance of the deep bidi-\n",
            "rectionality of BERT by evaluating two pre-\n",
            "training objectives using exactly the same pre-\n",
            "training data, ﬁne-tuning scheme, and hyperpa-\n",
            "rameters as BERT BASE :\n",
            "No NSP : A bidirectional model which is trained\n",
            "using the “masked LM” (MLM) but without the\n",
            "“next sentence prediction” (NSP) task. LTR & No NSP : A left-context-only model which\n",
            "is trained using a standard Left-to-Right (LTR)\n",
            "LM, rather than an MLM.\n",
            "1097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "semantic_chunk_vectorstore = Chroma.from_documents(semantic_chunks, embedding=embed_model)"
      ],
      "metadata": {
        "id": "5Z6fpNdHYNfQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will “limit” our semantic retriever to k = 1 to demonstrate the power of the semantic chunking strategy while maintaining similar token counts between the semantic and naive retrieved context."
      ],
      "metadata": {
        "id": "vg9AGGhUYWv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunk_retriever = semantic_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 1})"
      ],
      "metadata": {
        "id": "uQ5AW-mZYU6m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_chunk_retriever.invoke(\"Describe the Feature-based Approach with BERT?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t952hGTcYZy2",
        "outputId": "86b87658-fdf9-4108-a27a-4827b0acbf9e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3. From the table it can be seen that ﬁne-tuning is\\nsurprisingly robust to different masking strategies. However, as expected, using only the M ASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe R NDstrategy performs much worse than our\\nstrategy as well.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_template = \"\"\"\\\n",
        "Use the following context to answer the user's query. If you cannot answer, please respond with 'I don't know'.\n",
        "\n",
        "User's Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)"
      ],
      "metadata": {
        "id": "R5bKxXBNYaH_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat_model = ChatGroq(temperature=0,\n",
        "                      model_name=\"mixtral-8x7b-32768\",\n",
        "                      api_key=\"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\",)\n"
      ],
      "metadata": {
        "id": "WqoTHrwNfyq7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "semantic_rag_chain = (\n",
        "    {\"context\":semantic_chunk_retriever, \"question\":RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "igSyPLsIgCd6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_rag_chain.invoke(\"Describe the Feature-based Approach with BERT?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dN-6iw2GT5nR",
        "outputId": "0fb87429-1190-4af1-8b07-b0f746bac178"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Feature-based Approach with BERT involves concatenating the last 4 layers of BERT as features, which was found to be the most effective method in Section 5.3 of the paper. This approach is then compared in the table on the right side of the paper, which presents the Dev set results. Fine-tuning is shown to be robust to different masking strategies, but using only the MASK strategy is problematic when applying the feature-based approach to Named Entity Recognition (NER). Using only the RND strategy performs significantly worse than the proposed strategy.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_rag_chain.invoke(\"What is SQuADv2.0?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "LKZxOTjST61w",
        "outputId": "9bbca537-5151-49ae-fa33-69a9c7653e6e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SQuAD v2.0, or Squad 2.0, is a version of the Stanford Question Answering Dataset (SQuAD) that extends the problem definition of SQuAD 1.1 by allowing for the possibility that no short answer exists in the provided paragraph. This makes the problem more realistic. To extend the SQuAD v1.1 BERT model for this task, questions that do not have an answer are treated as having an answer span with start and end at the [CLS] token. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, the score of the no-answer span is compared to the score of the best non-null span. The TriviaQA data used for this task consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a RAG pipeline using Naive Chunking Strategy"
      ],
      "metadata": {
        "id": "_p1M4THST97B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_chunk_vectorstore = Chroma.from_documents(naive_chunks, embedding=embed_model)\n",
        "naive_chunk_retriever = naive_chunk_vectorstore.as_retriever(search_kwargs={\"k\" : 3})\n"
      ],
      "metadata": {
        "id": "7JLddwBlT75x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_rag_chain = (\n",
        "    {\"context\" : naive_chunk_retriever, \"question\" : RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "IEYphSS3dgxt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_rag_chain.invoke(\"Describe the Feature-based Approach with BERT?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "YW0M9JW2dkbN",
        "outputId": "432ae4aa-5a84-4951-c7b6-c9f2ae808141"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The feature-based approach with BERT involves extracting the activations from one or more layers of the pre-trained BERT model without fine-tuning any of its parameters. These contextual embeddings are then used as input to a separately initialized two-layer BiLSTM before the classification layer. The feature-based approach has been shown to perform competitively with state-of-the-art methods in Named Entity Recognition (NER) tasks. Specifically, the best performing feature-based method concatenates the token representations from the top four hidden layers of the pre-trained Transformer. This method is only 0.3 F1 behind fine-tuning the entire model, demonstrating the effectiveness of BERT for both fine-tuning and feature-based approaches. In the context of NER tasks, the feature-based approach concatenates the last 4 layers of BERT as the features, which was shown to be the best approach. Fine-tuning has been found to be surprisingly robust to different masking strategies, but using only the MASK strategy was problematic for the feature-based approach. Using only the RND strategy performed much worse than the proposed strategy as well.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_rag_chain.invoke(\"What is SQuADv2.0?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "9bzvx_wRdljY",
        "outputId": "e6e43479-039d-4eac-e712-8978424cfd1d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SQuADv2.0, or Squad 2.0, is a version of the Stanford Question Answering Dataset (SQuAD) that extends the problem definition of SQuAD 1.1 by allowing for the possibility that no short answer exists in the provided paragraph. This makes the problem more realistic. To handle such questions, a simple approach is used to extend the SQuAD v1.1 BERT model for this task. Questions that do not have an answer are treated as having an answer span with start and end at the [CLS] token, and the probability space for the start and end answer span positions is extended to include the position of the [CLS] token. Prediction is then made by comparing the score of the no-answer span to the score of the best non-null span. The document also mentions that systems are not allowed to use any public data when training, and modest data augmentation is used by ﬁne-tuning on TriviaQA before ﬁne-tuning on SQuAD.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ragas Assessment Comparison for Semantic Chunker"
      ],
      "metadata": {
        "id": "JoK7553CdowX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False\n",
        ")\n",
        "#\n",
        "synthetic_data_chunks = synthetic_data_splitter.create_documents([d.page_content for d in documents])\n",
        "print(len(synthetic_data_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvuliA4fdmoN",
        "outputId": "4bbaa8b7-df3f-484a-8bfb-4b706bc81df0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "ground_truths_semantic = []\n",
        "contexts = []\n",
        "answers = []\n",
        "\n",
        "question_prompt = \"\"\"\\\n",
        "You are a teacher preparing a test. Please create a question that can be answered by referencing the following context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "question_prompt = ChatPromptTemplate.from_template(question_prompt)\n",
        "\n",
        "ground_truth_prompt = \"\"\"\\\n",
        "Use the following context and question to answer this question using *only* the provided context.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "ground_truth_prompt = ChatPromptTemplate.from_template(ground_truth_prompt)\n",
        "question_chain = question_prompt | chat_model | StrOutputParser()\n",
        "ground_truth_chain = ground_truth_prompt | chat_model | StrOutputParser()\n",
        "for chunk in synthetic_data_chunks[10:20]:\n",
        "  questions.append(question_chain.invoke({\"context\" : chunk.page_content}))\n",
        "  contexts.append([chunk.page_content])\n",
        "  ground_truths_semantic.append(ground_truth_chain.invoke({\"question\" : questions[-1], \"context\" : contexts[-1]}))\n",
        "  answers.append(semantic_rag_chain.invoke(questions[-1]))"
      ],
      "metadata": {
        "id": "PXrH1gGsdtyF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "qagc_list = []\n",
        "\n",
        "for question, answer, context, ground_truth in zip(questions, answers, contexts, ground_truths_semantic):\n",
        "  qagc_list.append({\n",
        "      \"question\" : question,\n",
        "      \"answer\" : answer,\n",
        "      \"contexts\" : context,\n",
        "      \"ground_truth\" : ground_truth\n",
        "  })\n",
        "\n",
        "eval_dataset = Dataset.from_list(qagc_list)\n",
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Robc9nydfJLE",
        "outputId": "0ed89a08-6ff8-41f9-bd73-400f49146b62"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVVQod3ffOoM",
        "outputId": "dd7f1ee2-0647-4143-f9ef-a7f1a169501c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Question: Based on the given context, what are the special symbols added in front of every input example and separating questions/answers in BERT?\\n\\nAnswer: The special symbols added in front of every input example in BERT is [CLS], and [SEP] is the special separator token used to separate questions/answers.',\n",
              " 'answer': 'The special symbols added in front of every input example in BERT is [CLS], and [SEP] is the special separator token used to separate questions/answers.',\n",
              " 'contexts': ['BERT BERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nQuestion Paragraph Start/End Span \\nBERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nMasked Sentence A Masked Sentence B \\nPre-training Fine-Tuning NSP Mask LM Mask LM \\nUnlabeled Sentence A and B Pair SQuAD \\nQuestion Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,'],\n",
              " 'ground_truth': 'The special symbols added in front of every input example in BERT is [CLS], and [SEP] is the special separator token used to separate questions/answers. This information is provided in the context in the sentence \"The same pre-trained model parameters are used to initialize models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-tions/answers).\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import (\n",
        "    answer_relevancy,\n",
        "    faithfulness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")"
      ],
      "metadata": {
        "id": "jns-3XXKgLgW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "\n",
        "result = evaluate(\n",
        "    eval_dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "        context_recall,\n",
        "    ],\n",
        "     llm=chat_model,\n",
        "    embeddings=embed_model,\n",
        "    raise_exceptions=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eUOB7fL_gXgG",
        "outputId": "a7ad5bb0-a955-4aab-caf4-fe6709ce8c41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eval_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b21a5082e9d7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m result = evaluate(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     metrics=[\n\u001b[1;32m      6\u001b[0m         \u001b[0mcontext_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxYQY40HgZF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}